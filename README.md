# On the Effectiveness and Generalization of Race Representations for Debiasing High-Stakes Decisions

## Overview
<!-- This project focuses on analyzing and mitigating prediction biases in large language models (LLMs). It includes tools for loading, saving, and intervening in model predictions, as well as utilities for alignment and bias evaluation. -->
This project features the code for the experiments used in our paper, On the Effectiveness and Generalization of Race Representations for Debiasing High-Stakes Decisions, which uncovers the mechanism behind LLMs' racial biases and performs interventions to mitigate them. Check out the full paper [here](https://arxiv.org/abs/2504.06303)!

## Relevant files and folders in this codebase
```
llm_prediction_bias/
├── model_decision_analysis.py # Script for analyzing model decisions
├── make_controlled_data.py  # Script for creating controlled datasets to measure models' biases
├── auto_ctf.py              # Script for automated counterfactual dataset generation
├── train_alignment.py       # Script for training alignments
├── cross_task_intervention.py # Script for cross-task interventions
├── plot_iia.py              # Script for plotting the interchange intervention accuracy (IIA) heatmap after training
├── prompts/                 # Directory for prompt templates
```

## Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/ChicagoHAI/llm-prediction-bias.git
   cd llm_prediction_bias
   ```

2. Install dependencies:
   ```bash 
   pip install -r requirements.txt
   ```

## Usage

### `model_decision_analysis.py`
This script generates the Admissions and Hiring datasets and collects the model's decision for each applicant profile. It also plots the histograms of acceptances by feature, e.g., number of acceptances by GPA.

Here's an example of how you can use it. Feel free to change the directory names to suit your use case.
```bash
python model_decision_analysis.py \
    --dataset_size 2000 \
    --batch_size 128 \
    --task AdmissionsNames \
    --model_name meta-llama/Llama-3.2-3B-Instruct \
    --template_path ./prompts/admissions-race-prompting/Meta-Llama-3.2-3B-Instruct/admissions_race_name.txt \
    --preds_save_path ./datasets/admissions-race-prompting/Meta-Llama-3.2-3B-Instruct \
    --probs_save_path ./results/admissions-race-prompting/Meta-Llama-3.2-3B-Instruct \
    --plots_save_path ./plots/admissions-race-prompting/Meta-Llama-3.2-3B-Instruct
```

### `auto_ctf.py`
This script generates counterfactual datasets automatically using the base and source datasets generated by `model_decision_analysis.py`. Generally, base and source datasets can have different prompt formats. We show a simple case where the prompt formats are the same in order to later train alignments.

Example:
```bash
python auto_ctf.py \
    --source_path ./datasets/admissions-race-prompting/Meta-Llama-3.2-3B-Instruct/name/preds.csv \
    --base_path ./datasets/admissions-race-prompting/Meta-Llama-3.2-3B-Instruct/name/preds.csv \
    --model_name llama3 \
    --causal_variable name \
    --side_variables uni gpa num_letters num_ecs \
    --train_dev_split 0.6 0.2 \
    --save_path ./datasets/admissions-race-prompting_name_autoctf/Meta-Llama-3.2-3B-Instruct
```

### `train_alignment.py`
With the counterfactual dataset in hand, we can now train alignments. We get to decide between Vanilla or Boundless Distributed Alignment Search, which searches for a subspace with a specified dimension or automatically optimizes the dimension, respectively. Specify this using either 'das' or 'bdas' in `--intervention-type`. If you use `das`, then you need to further specify the dimension in `interchange_dim`.

`horizontal_start`, `horizontal_end` specify the token range over which to search for alignments, and `vertical_start`, `vertical_end` specify the layer range. `extra_steps` specifies how many steps before `horizontal_start` to search over, and optionally how many steps before the last token, depending on whether `train_end` is set or not.

Example:
```bash
python train_alignment.py \
    --dataset_path ./datasets/admissions-race-prompting_name_autoctf/Meta-Llama-3.2-3B-Instruct \
    --model_name meta-llama/Meta-Llama-3.2-3B-Instruct \
    --intervention_type das \
    --interchange_dim 1000 \
    --num_epochs 1 \
    --n_train 2000 \
    --n_dev -1 \
    --batch_size 32 \
    --horizontal_start -69 \
    --horizontal_end -70 \
    --horizontal_step 1 \
    --extra_steps 0 \
    --train_end \
    --vertical_start 10 \
    --vertical_end 11 \
    --vertical_step 1 \
    --models_save_path ./alignments/admissions-race-prompting_name_autoctf/Meta-Llama-3.2-3B-Instruct \
    --results_save_path ./results/admissions-race-prompting_name_autoctf/Meta-Llama-3.2-3B-Instruct \
    --save_alignments
```

### `cross_task_intervention.py`
Once we have a good alignment, we can perform *cross-task interventions* to verify its generalizability across prompts, tasks, etc. This script can also be used to compute the alignment's performance on a held-out test set.

Example:
```bash
python cross_task_intervention.py \
    --n_test -1 \
    --model_name meta-llama/Meta-Llama-3.2-3B-Instruct \
    --dataset_path ./datasets/admissions-race-prompting_name_autoctf/Meta-Llama-3.2-3B-Instruct \
    --save_path ./results/admissions-race-prompting_name_autoctf/Meta-Llama-3.2-3B-Instruct/das-500-name_collect_10.-1_patch_10.-1 \
    --intervention interchange \
    --interchange_dim 500 \
    --method das \
    --batch_size 32 \
    --collect_layer 10 \
    --collect_token -1 \
    --patch_start 10 \
    --patch_end 10 \
    --patch_tokens custom \
    --patch_token_positions -1
```

### `make_controlled_data.py`
Similar to `model_decision_analysis.py`, this scripts creates a decision dataset. The difference is it controls for applicants' qualifications and is meant for testing debiasing methods. For each sampled set of qualifications (e.g., GPA, letters, etc.), we vary the applicant's race in order to evaluate racial biases. 

Example:
```bash
python make_controlled_data.py \
    --n_samples 400 \
    --src_task AdmissionsNames \
    --base_task AdmissionsNames \
    --src_template_path ./prompts/admissions-race-prompting/Meta-Llama-3.2-3B-Instruct/admissions_race_name.txt \
    --base_template_path ./prompts/admissions-race-prompting/Meta-Llama-3.2-3B-Instruct/admissions_race_name.txt \
    --save_path ./datasets/admissions-name_credentials-controlled/Meta-Llama-3.2-3B-Instruct
```

### `evaluate_debiasing.py`


### `plot_iia.py`
This script generates IIA (Independence of Irrelevant Alternatives) plots.

Example:
```bash
python plot_iia.py \
    --model_name /data/transformers/Meta-Llama-3.2-3B-Instruct \
    --results_path ./results/admissions-race-prompting_name_autoctf_das-500_n-train-2000_all-pos/Meta-Llama-3.2-3B-Instruct/test \
    --dataset_path ./datasets/admissions-race-prompting_name_autoctf/Meta-Llama-3.2-3B-Instruct \
    --horizontal_start -92 \
    --horizontal_end -85 \
    --horizontal_step 1 \
    --extra_steps 2 \
    --vertical_start 0 \
    --vertical_end 17 \
    --vertical_step 2 \
    --save_file ../plots/overleaf_plots/admissions-names_Meta-Llama-3.2-3B-Instruct_all-unis_test_iia
```

## Contact
For questions or feedback, please contact [dangnguyen@uchicago.edu].